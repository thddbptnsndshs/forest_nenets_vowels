{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d3f0bce",
   "metadata": {},
   "source": [
    "# duration data analysis\n",
    "\n",
    "TODO:\n",
    "\n",
    "- DONE walk the directory with textgrids\n",
    "- DONE extract textgrids\n",
    "- DONE collect all instances of vowels with contexts and speaker names\n",
    "- find and correct spelling errors in words\n",
    "- split palatalized and non-palatalized contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb413e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textgrid\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import os, sys\n",
    "from time import strftime, localtime\n",
    "\n",
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29e1b1f",
   "metadata": {},
   "source": [
    "## reading files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3067ebd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = '../misc/'\n",
    "AUDIO_TEXTGRID_PATH = '../misc/audio/' # wav + TextGrid\n",
    "FORMANT_PATH = '../misc/formants/' # csv formant data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d094db64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename files to swap spaces for underscores\n",
    "\n",
    "for path, subdirs, files in os.walk('./misc/audio/'):\n",
    "    for name in files:\n",
    "        fn = os.path.join(path, name)\n",
    "#         print(fn)\n",
    "        new_fn = re.sub(' ', '_', fn)\n",
    "#         print(new_fn)\n",
    "#         print()\n",
    "        os.rename(fn, new_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2803ef33",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -r ./misc/formants/.DS_Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a295bdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv outputs\n",
    "\n",
    "# df = pd.concat([\n",
    "#     pd.read_csv(os.path.join(FORMANT_PATH, fn)) for fn in os.listdir(FORMANT_PATH)\n",
    "# ]).dropna(subset='phoneme').reset_index(drop=True)\n",
    "\n",
    "df = pd.read_csv(os.path.join(FORMANT_PATH, 'output_20240723.csv'), delimiter='\\t')\n",
    "\n",
    "# extract means from formants data\n",
    "\n",
    "for col in ['f0', 'f1', 'f2', 'f3']:\n",
    "    df[col.upper()] = df[col].apply(lambda x: np.array([float(val) for val in x[2:-1].split(',')]).mean())\n",
    "    \n",
    "df = df[['filename', 'word', 'segment', 'vowelIntervalNum', 'wordIntervalNum',\n",
    "       'duration', 'f0max', 'f0min', 'intensity', 'intensity_max', 'F0', 'F1', 'F2', 'F3']]\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6086f4f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mrg_df = df.dropna(subset='word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c780c001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is how many contexts we'll have to resolve\n",
    "(mrg_df[['word', 'segment']].value_counts() > 1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536e0ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mrg_df['word'] = mrg_df['word'].apply(lambda x: x.split())\n",
    "mrg_df = mrg_df.explode('word').reset_index(drop=True)\n",
    "mrg_df['word'] = mrg_df['word'].apply(lambda x: 'ʔ' + x if x[0] in 'aeoiuæăĕŏĭŭ°æy' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6eb512",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mrg_df.shape\n",
    "# mrg_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dedea7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill NaN's automatically when possible\n",
    "\n",
    "consonants = 'mḿnńŋŋ́pṕtčkʔʰsšxx́λlwẃjd' + 'bcfghqrv'\n",
    "vowels = 'aeoiuæăĕŏĭŭ°æ' + 'y'\n",
    "vowels_short = 'ăĕŏĭŭ°æ̆'\n",
    "vowels_long = 'aeoiuæ'\n",
    "\n",
    "def parse_syllables(word):\n",
    "    if word[0] in vowels:\n",
    "        word = 'ʔ' + word # no vowel-initial words\n",
    "    cv_mask = ''\n",
    "    one2one_parse = ''\n",
    "    # create a CV-mask\n",
    "    for seg in word:\n",
    "        if seg in '́ ʹ':\n",
    "            continue\n",
    "        elif seg in consonants:\n",
    "            cv_mask += 'C'\n",
    "        elif seg in vowels_short:\n",
    "            cv_mask += 'v'\n",
    "        elif seg in vowels_long:\n",
    "            cv_mask += 'V'\n",
    "        one2one_parse += seg\n",
    "        \n",
    "    # split into syllables\n",
    "    nucleus = False\n",
    "    coda = False\n",
    "    onset = False\n",
    "    syllables = list()\n",
    "    syll, seg_syll = '', ''\n",
    "    for slot, seg in zip(cv_mask[::-1], one2one_parse[::-1]):\n",
    "        syll += slot.replace('V', 'VV').replace('v', 'V')\n",
    "        seg_syll += seg\n",
    "        if slot == 'C':\n",
    "            if nucleus:\n",
    "                syllables.append((syll[::-1], seg_syll[::-1]))\n",
    "                syll, seg_syll = '', ''\n",
    "                nucleus, coda, onset = False, True, False\n",
    "        else:\n",
    "            nucleus = True\n",
    "    return syllables[::-1]\n",
    "                \n",
    "    \n",
    "words = ['ańa', 'xălakuhkon°tă', 'kăpčaḿṕoš°tu', 'tŭ', 'tol°', 'paŋk', 'taŋksa', 'pĭt kaλ´a'] \n",
    "print(*[parse_syllables(word) for word in words], sep='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e314ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_context(syllables, vowel):\n",
    "    \"\"\"\n",
    "    get the first occurrence of vowel and return the context characteristics\n",
    "    takes syllables of form [(cv_mask, segments)*]\n",
    "    returns a tuple of (syllable structure, syllable count, position, stress, vowel)\n",
    "    \"\"\"\n",
    "    target_syll = None\n",
    "    for idx, (syll, seg_syll) in enumerate(syllables):\n",
    "        if vowel in seg_syll:\n",
    "            target_syll, target_seg_syll = syll, seg_syll\n",
    "            target_idx = idx\n",
    "            break\n",
    "    # return empty spaces if the vowel is not found\n",
    "    if target_syll == None:\n",
    "        return ('', '', '', '', '', '')\n",
    "    \n",
    "    syllable_structure = target_syll\n",
    "    syllable_count = 'monosyllable' if len(syllables) == 1 else 'polysyllabic'\n",
    "    \n",
    "    if syllable_count == 'monosyllable':\n",
    "        position = 'final'\n",
    "    else:\n",
    "        if target_idx == 0:\n",
    "            position = 'initial'\n",
    "        elif target_idx == len(syllables) - 1:\n",
    "            position = 'final'\n",
    "        else:\n",
    "            position = 'medial'\n",
    "            \n",
    "    stress = 'stressed' if idx % 2 == 0 and (position != 'final' or syllable_count == 'monosyllable') else 'unstressed'\n",
    "    if vowel in 'aă':\n",
    "        vowel = 'low'\n",
    "    elif vowel in 'uŭiĭ':\n",
    "        vowel = 'high'\n",
    "    elif vowel in 'eĕoŏææ̆':\n",
    "        vowel = 'mid'\n",
    "        \n",
    "    if position != 'final':\n",
    "        if '°' in syllables[target_idx + 1][1]:\n",
    "            pre_schwa = 'yes'\n",
    "        else:\n",
    "            pre_schwa = 'no'\n",
    "    else:\n",
    "        pre_schwa = 'no'\n",
    "        \n",
    "    return syllable_structure, syllable_count, position, stress, vowel, pre_schwa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262a2dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_contexts(syllables, vowels, indices):\n",
    "    \"\"\"\n",
    "    vowels: list of (V, List[str, float])\n",
    "    get the first occurrence of vowel and return the context characteristics\n",
    "    takes syllables of form [(cv_mask, segments)*]\n",
    "    returns a tuple of (syllable structure, syllable count, position, stress, vowel)\n",
    "    \"\"\"\n",
    "    target_syll = None\n",
    "    out = list()\n",
    "    \n",
    "    def get_context(vowel, target_syll, target_seg_syll, target_idx):\n",
    "        \n",
    "        if target_syll == None:\n",
    "            return ('', '', '', '', '', '')\n",
    "\n",
    "        syllable_structure = target_syll\n",
    "        syllable_count = 'monosyllable' if len(syllables) == 1 else 'polysyllabic'\n",
    "\n",
    "        if syllable_count == 'monosyllable':\n",
    "            position = 'final'\n",
    "        else:\n",
    "            if target_idx == 0:\n",
    "                position = 'initial'\n",
    "            elif target_idx == len(syllables) - 1:\n",
    "                position = 'final'\n",
    "            else:\n",
    "                position = 'medial'\n",
    "\n",
    "        stress = 'stressed' if idx % 2 == 0 and (position != 'final' or syllable_count == 'monosyllable') else 'unstressed'\n",
    "        if vowel in 'aă':\n",
    "            vowel = 'low'\n",
    "        elif vowel in 'uŭiĭ':\n",
    "            vowel = 'high'\n",
    "        elif vowel in 'eĕoŏææ̆':\n",
    "            vowel = 'mid'\n",
    "\n",
    "        if position != 'final':\n",
    "            if '°' in syllables[target_idx + 1][1]:\n",
    "                pre_schwa = 'yes'\n",
    "            else:\n",
    "                pre_schwa = 'no'\n",
    "        else:\n",
    "            pre_schwa = 'no'        \n",
    "\n",
    "        return syllable_structure, syllable_count, position, stress, vowel, pre_schwa\n",
    "    \n",
    "    while len(vowels) > 0:\n",
    "        for idx, (syll, seg_syll) in enumerate(syllables):\n",
    "            for i, vowel in enumerate(vowels):\n",
    "#                 print(vowels, indices)\n",
    "                if vowel not in ''.join([s[1] for s in syllables]):\n",
    "#                     print()\n",
    "                    vowels.pop(i)\n",
    "                    indices.pop(i)\n",
    "                if vowel in seg_syll:\n",
    "#                     print(vowel, seg_syll)\n",
    "                    target_syll, target_seg_syll = syll, seg_syll\n",
    "                    target_idx = idx\n",
    "                    out.append(\n",
    "                        [vowel,\n",
    "                         indices[i],\n",
    "                        get_context(vowel, target_syll, target_seg_syll, target_idx)]\n",
    "                    )\n",
    "                    vowels.pop(i)\n",
    "                    indices.pop(i)\n",
    "                    break\n",
    "                            \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb52f2c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "words = ['xălakuhkon°tă', 'kăpčaḿṕoš°tu', 'tŭ', 'tol°', 'paŋk', 'aŋksa'] \n",
    "vowels = [['ă', 'a', 'u', 'o', 'ă'], ['o', 'a'], ['ŭ', 'ŭ', 'ŭ'], ['o'], ['e'], ['a', 'a'],]\n",
    "indices = [[1, 2, 3, 4, 5], [1, 2], [2, 2, 2], [1], [2], [444, 44]]\n",
    "\n",
    "for w, v, i in zip(words, vowels, indices):\n",
    "    print(parse_syllables(w))\n",
    "    print(*determine_contexts(parse_syllables(w), v, i), sep='\\n\\n')\n",
    "    print('================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e77691",
   "metadata": {},
   "outputs": [],
   "source": [
    "mrg_df_filled = mrg_df.copy()\n",
    "for idx, row in mrg_df.iterrows():\n",
    "    syllable_structure, syllable_count, position, stress, vowel, pre_schwa = determine_context(\n",
    "        parse_syllables(\n",
    "            row.word\n",
    "        ), row.segment\n",
    "    )\n",
    "#     mrg_df_filled.loc[idx, ['word', 'segment']] =\n",
    "    mrg_df_filled.loc[idx, ['nenets', \n",
    "                            'syllable structure', \n",
    "                            'syllable count', \n",
    "                            'position', \n",
    "                            'stress', \n",
    "                            'vowel',\n",
    "                            'pre-schwa']\n",
    "                     ] = row.word, syllable_structure, syllable_count, position, stress, vowel, pre_schwa\n",
    "mrg_df_filled['consultant'] = mrg_df_filled['filename'].apply(lambda x: x.split('_')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42737439",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mrg_df_grouped = pd.DataFrame(mrg_df_filled.groupby(['filename', 'word',])['segment'].apply(lambda x: list(x))).reset_index() \n",
    "mrg_df_grouped['indeces'] = mrg_df_filled.reset_index().groupby(['filename', 'word'])['index']\\\n",
    "    .apply(lambda x: list(x)).values\n",
    "# mrg_df_grouped\n",
    "res = []\n",
    "\n",
    "# 'vowelIntervalNum', 'wordIntervalNum',\n",
    "\n",
    "for idx, row in mrg_df_grouped.iterrows():\n",
    "    res.append(\n",
    "        determine_contexts(\n",
    "            parse_syllables(\n",
    "                row.word\n",
    "            ), row.segment, row.indeces\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb987227",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mrg_df_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5248fb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "mrg_df_grouped['out'] = res\n",
    "mrg_df_grouped_expl = mrg_df_grouped.explode(['out'])\n",
    "mrg_df_grouped_expl[['segment', 'indeces', 'context',]] = mrg_df_grouped_expl['out'].apply(pd.Series).values\n",
    "mrg_df_grouped_expl[['syllable structure', \n",
    "                    'syllable count', \n",
    "                    'position', \n",
    "                    'stress', \n",
    "                    'vowel',\n",
    "                    'pre-schwa']] = mrg_df_grouped_expl['context'].apply(pd.Series).values\n",
    "mrg_df_grouped_expl = mrg_df_grouped_expl.drop(columns=['out', 'context'])\n",
    "mrg_df_grouped_expl['consultant'] = mrg_df_grouped_expl['filename'].apply(lambda x: x.split('_')[-1])\n",
    "# mrg_df_grouped_expl.indeces.value_counts()\n",
    "# mrg_df_grouped_expl = mrg_df_grouped_expl.drop(['vowelIntervalNum', 'wordIntervalNum']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f71f274",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mrg_df_grouped_expl.loc[mrg_df_grouped_expl.word == 'kata']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f18d65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mrg_df_filled = mrg_df_grouped_expl.merge(mrg_df, left_on='indeces', right_index=True, suffixes=('', '_DROP'))\n",
    "mrg_df_filled = mrg_df_filled.drop(columns=list(filter(lambda x: '_DROP' in x, mrg_df_filled.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54882ebb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# this is how many contexts we'll have to add by hand\n",
    "# now that's better\n",
    "(mrg_df_filled['word'] == '').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f91d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "mrg_df_filled['syllable structure'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1139daa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove parsing errors\n",
    "\n",
    "mrg_df_filled = mrg_df_filled.loc[~mrg_df_filled['syllable structure'].isin(['CVCC', 'CVCC', 'CVVCCC'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff1fa5c",
   "metadata": {},
   "source": [
    "## let's draw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15dfd9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mrg_df_filled.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eeba971",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mrg_df_filled.shape)\n",
    "mrg_df_filled.loc[mrg_df_filled.vowel != ''].drop_duplicates(subset=['word', 'segment', 'filename']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa7e8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = mrg_df_filled.loc[(mrg_df_filled.vowel != '') & (mrg_df_filled['syllable count'] == 'monosyllable')].drop_duplicates(subset=['word', 'segment', 'filename'])\n",
    "data.F1 = data.F1.astype(float)\n",
    "data.F2 = data.F2.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078ca1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove outliers by duration\n",
    "# data = data.loc[(data.duration > data.duration.quantile(.1)) & (data.duration < data.duration.quantile(.9))]\n",
    "\n",
    "# sample certain vowels\n",
    "# data = data.loc[data.vowel == 'mid']\n",
    "\n",
    "# just AOK\n",
    "data = data.loc[\n",
    "#         (data['syllable count'] == 'polysyllabic') & \\\n",
    "#         (data.vowel == 'low') & \\\n",
    "        (~data.segment.isin(['e'])) & \\\n",
    "        (data.consultant == 'AOK')\n",
    "]\n",
    "\n",
    "# leave out tsAYuU\n",
    "# data = data.loc[(~data.file.str.contains('_tsAYuU')) & (data.position == 'initial')]\n",
    "\n",
    "data.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082fbfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "\n",
    "x_name = 'F2'\n",
    "y_name = 'F1'\n",
    "\n",
    "x = data[x_name]\n",
    "y = data[y_name]\n",
    "\n",
    "ax.scatter(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bede185",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = cm.get_cmap('Dark2')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "\n",
    "x_name = 'F2'\n",
    "y_name = 'F1'\n",
    "\n",
    "x = data[x_name]\n",
    "y = data[y_name]\n",
    "\n",
    "ax.scatter(x, y,marker=\"\")\n",
    "\n",
    "for v, color in zip(data.segment.unique(),cmap.colors):\n",
    "    X = data[x_name].loc[data.segment == v]\n",
    "    Y = data[y_name].loc[data.segment == v]\n",
    "    for x, y in zip(X,Y):\n",
    "        ax.annotate(v,(x,y),fontsize=14,color=color)\n",
    "\n",
    "ax.invert_xaxis()\n",
    "ax.invert_yaxis()\n",
    "ax.set_xlabel(x_name,fontsize=16)\n",
    "ax.set_ylabel(y_name,fontsize=16)\n",
    "ax.yaxis.tick_right()\n",
    "ax.xaxis.tick_top()\n",
    "ax.yaxis.set_label_position(\"right\")\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "ax.set_title('Vowels',fontsize=18)\n",
    "#ax.grid()\n",
    "#plt.savefig('my_vowel_plot.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db01ffd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tease apart outliers\n",
    "data.loc[(data.F2 > 1300) & (data.F1 > 400) & (data.segment == 'ŭ')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba7cec1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot duration distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722e101b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_data = mrg_df_filled.loc[\n",
    "        (mrg_df_filled['syllable count'] == 'polysyllabic') & \\\n",
    "        (mrg_df_filled.vowel == 'low') & \\\n",
    "#         (mrg_df_filled.position != 'final') & \\\n",
    "        (mrg_df_filled.consultant == 'AOK')]\\\n",
    "    .groupby(['syllable structure', 'position', 'stress'])['duration'].mean().round(5) * 1000\n",
    "\n",
    "a_data = pd.DataFrame(a_data)\n",
    "a_data['std'] = (mrg_df_filled.loc[(mrg_df_filled['syllable count'] == 'polysyllabic') & (mrg_df_filled.vowel == 'low')]\\\n",
    "    .groupby(['syllable structure', 'position', 'stress'])['duration'].std().round(5) * 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e60993",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "a_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b280d801",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data.iloc[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d00d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# non-final length distributions\n",
    "\n",
    "plot_data = mrg_df_filled.loc[\n",
    "    (mrg_df_filled['syllable count'] == 'polysyllabic') & \\\n",
    "    (mrg_df_filled.vowel != 'mid') & \\\n",
    "    (mrg_df_filled['pre-schwa'] == 'no') & \\\n",
    "    (mrg_df_filled.position != 'final')\n",
    "]\n",
    "plot_data.loc[len(plot_data)] = ['', '', 'ă', 0, 'CVC',\n",
    "       'polysyllabic', 'medial', 'unstressed', 'low', 'no', 'AOK', 0, 0,\n",
    "       0, 0, 0, 0, 0, 0, 0, 0, 0,]\n",
    "\n",
    "g = sns.FacetGrid(plot_data,\n",
    "                  row='position', col='syllable structure', margin_titles=True, height=3, aspect=1,)\n",
    "g.map(sns.barplot, 'stress', \"duration\", palette=\"Set1\")\n",
    "g.add_legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcca0a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final length\n",
    "\n",
    "plot_data = mrg_df_filled.loc[\n",
    "    (mrg_df_filled['syllable count'] == 'polysyllabic') & \\\n",
    "    (mrg_df_filled.vowel == 'low') & \\\n",
    "    (mrg_df_filled['syllable structure'] != 'CVVCC') & \\\n",
    "    (mrg_df_filled.position == 'final')\n",
    "]\n",
    "\n",
    "g = sns.FacetGrid(plot_data,\n",
    "                  row='vowel', col='syllable structure', margin_titles=True, height=3, aspect=1,)\n",
    "g.map(sns.barplot, 'stress', \"duration\", palette='pastel')\n",
    "g.add_legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b909dbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data.groupby(['syllable structure', 'stress', 'vowel'])['duration'].mean()\n",
    "# plot_data.loc[plot_data['syllable structure'] == 'CVVC'].sort_values('duration', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b1b6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# monosyllables by vowel quality\n",
    "\n",
    "plot_data = mrg_df_filled.loc[\n",
    "    (mrg_df_filled['syllable count'] == 'monosyllable') & \\\n",
    "    (mrg_df_filled['syllable structure'] != 'CVVCC') & \\\n",
    "    (mrg_df_filled['syllable structure'] != 'CVCC')\n",
    "]\n",
    "plot_data.loc[len(plot_data)] = ['', '', 0, 0, 'ă', 0, 'CV',\n",
    "       'monosyllabic', 'medial', 'unstressed', 'low', 'no', 'AOK',\n",
    "       0, 0, 0, 0, 0, 0, 0, 0, 0,]\n",
    "\n",
    "g = sns.FacetGrid(plot_data,\n",
    "                  col='syllable structure', margin_titles=True, height=3, aspect=1,)\n",
    "g.map(sns.barplot, 'vowel', \"duration\", palette='pastel6')\n",
    "g.add_legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35524f23",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# pre-schwa unstressed vs stressed\n",
    "\n",
    "plot_data = mrg_df_filled.loc[\n",
    "    (mrg_df_filled['syllable count'] == 'polysyllabic') & \\\n",
    "    (mrg_df_filled['syllable structure'] != 'CVVCC') & \\\n",
    "    (mrg_df_filled['syllable structure'] != 'CVCC')\n",
    "]\n",
    "# plot_data.loc[len(plot_data)] = ['', '', 0, 0, 'ă', 0, 'CVC',\n",
    "#        'monosyllabic', 'medial', 'unstressed', 'low', 'yes', 'AOK',\n",
    "#        0, 0, 0, 0, 0, 0, 0, 0, 0,]\n",
    "plot_data.loc[len(plot_data)] = ['', '', 'ă', 0, 'CVVC',\n",
    "       'polysyllabic', 'medial', 'unstressed', 'low', 'yes', 'AOK', 0, 0,\n",
    "       0, 0, 0, 0, 0, 0, 0, 0, 0,]\n",
    "\n",
    "g = sns.FacetGrid(plot_data,\n",
    "                  col='syllable structure', row='stress', margin_titles=True, height=3, aspect=1,)\n",
    "g.map(sns.barplot, 'pre-schwa', \"duration\", palette='flare')\n",
    "g.add_legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fba16fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# monosyllabic vs polysyllabic short\n",
    "\n",
    "plot_data = mrg_df_filled.loc[\n",
    "    (\n",
    "        mrg_df_filled['syllable structure'].isin(['CVC', 'CV']) & \\\n",
    "        (mrg_df_filled['syllable count'] == 'polysyllabic') & \\\n",
    "        (mrg_df_filled['stress'] == 'stressed')\n",
    "    )\n",
    "    | \\\n",
    "    (\n",
    "        (mrg_df_filled['syllable count'] == 'monosyllable') & \\\n",
    "        mrg_df_filled['syllable structure'].isin(['CVC', 'CV'])\n",
    "    )\n",
    "]\n",
    "# plot_data.loc[len(plot_data)] = ['', '', 0, 0, 'ă', 0, 'CVC',\n",
    "#        'monosyllabic', 'medial', 'unstressed', 'low', 'yes', 'AOK',\n",
    "#        0, 0, 0, 0, 0, 0, 0, 0, 0,]\n",
    "# plot_data.loc[len(plot_data)] = ['', '', 'ă', 0, 'CVVC',\n",
    "#        'polysyllabic', 'medial', 'unstressed', 'low', 'yes', 'AOK', 0, 0,\n",
    "#        0, 0, 0, 0, 0, 0, 0, 0, 0,]\n",
    "\n",
    "g = sns.FacetGrid(plot_data,\n",
    "                  col='syllable structure', row='vowel', margin_titles=True, height=3, aspect=1,)\n",
    "g.map(sns.barplot, 'syllable count', \"duration\", palette='flare')\n",
    "g.add_legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831c2f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check specific word\n",
    "\n",
    "word = 'dʹa'\n",
    "print('\\n', word)\n",
    "\n",
    "plot_data = mrg_df_filled.loc[\n",
    "    (mrg_df_filled['word'] == word)\n",
    "]\n",
    "\n",
    "g = sns.FacetGrid(plot_data,\n",
    "                  row='syllable structure', col='stress', margin_titles=True, height=3, aspect=1)\n",
    "g.map(sns.barplot, 'pre-schwa', \"duration\", palette='flare')\n",
    "g.add_legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b00b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw a duration table for every vowel in the word\n",
    "\n",
    "def get_word_duration_table(word, round_factor=5, columns_to_add=['position', 'stress']):\n",
    "#  & (~mrg_df_filled.filename.str.contains('kăm')\n",
    "    grpb_object = mrg_df_filled.loc[(mrg_df_filled.word == word)].groupby([\n",
    "        'word', 'segment',\n",
    "        'syllable structure',\n",
    "        'position', 'stress', \n",
    "        'vowel', 'pre-schwa',\n",
    "    ])['duration']\n",
    "    mean, std, count = grpb_object.mean(), grpb_object.std(), grpb_object.count()\n",
    "    table = mean.rename('mean, ms').to_frame()\\\n",
    "        .join(std.rename('std, ms'))\\\n",
    "        .round(round_factor) * 1000\n",
    "    table = table.join(count.rename('count'))\n",
    "    table = table.reset_index()[['word', 'segment', 'mean, ms', 'std, ms', 'count'] + columns_to_add]\n",
    "    print(table, end='\\n\\n')\n",
    "    print(table.to_latex(index=False))\n",
    "    \n",
    "get_word_duration_table('kemta')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
